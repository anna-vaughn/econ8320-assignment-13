{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Customization vs Rapid Development\n",
    "\n",
    "As we know from (painful?) experience, Python is powerful because of its ability to leverage `numpy` and `scipy` to implement any statistical model from scratch. We can write the requisite matrix algebra, or the relevant likelihood function, and from there can optimize our model, calculate confidence intervals, and report the output of that model through data frames, lists, or printed tables. Building our own models is great! We get to build a model based on the exact context and assumptions of our problem, and therefore get exactly the model that we wanted. Unfortunately, it takes a LOT of time!\n",
    "\n",
    "This lesson will provide our first exposure to pre-written statistical modeling in Python. We will be able to use only a couple of lines of code to implement complex and valuable statistical and machine learning models. Because the most costly asset in programming is the time that we spend debugging and writing code (running code is MUCH faster and cheaper than the time spent writing code), we are always looking for ways to avoid writing code that someone else has already written.\n",
    "\n",
    "`statsmodels` is a library that covers the majority of regression models commonly used by economists and statisticians in other fields.\n",
    "\n",
    "`sklearn` is an analogous library that covers machine learning models (aside from deep neural networks, which have their own implementations).\n",
    "\n",
    "Each of these libraries is highly optimized to provide performant implementations of models that we use regularly, and allow us to avoid writing these models from scratch unless we need to customize our model for some specific use case! This is great news! You'll never have to think about writing your own linear or logistic regression from scratch again!\n",
    "\n",
    "Let's dive in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` makes statistics in Python easy! The library contains tools for regressions ranging from linear regression, to logistic regression, count regressions (negative binomial and poisson), various options for robust covariance measures, and tools to implement time series models as well! There are also really useful tools for assisting in creating our regression model based on any structure that best suits us.\n",
    "\n",
    "We can import `statsmodels` in one of two ways:\n",
    "\n",
    "1) With support for R-style formulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
    "      import pandas.util.testing as tm\n",
    "\n",
    "\n",
    "This is probably the best way to import our data if we are doing regression analysis for causal inference. In these cases, we are not typically trying to make predictions as new data arrives, and so we do not need to have tools ready to analyze new data using our existing regression models.\n",
    "\n",
    "2) Import `statsmodels` to use pre-built numpy arrays as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have other tools that we can use, but we need to manually arrange our `x` and `y` matrices. It looks clunky at first, but can be useful when we are building predictive pipelines using regression models, or when we might want to use both `statsmodels` and `sklearn` with the same data source.\n",
    "\n",
    "Let's start with option 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using formulas, we prepare our dataset by importing the data into a Pandas `DataFrame`. We should take care that each of our variables has a name with \n",
    "1) **No spaces**\n",
    "2) No symbols\n",
    "3) Made up of letters and numbers (also can't have a number as the first character)\n",
    "\n",
    "Our code so far might look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that our data set has already been cleaned. If our data has not yet been cleaned, then we need to clean our data prior to working with either `statsmodels` or `sklearn`. This is because regression AND machine learning models require that all information be provided in numeric format. We need to transform text-based data into categorical data (using either ordered numeric columns or binary variable columns generated from our categories), and ensure that all data is represented in the way that we want to use it within our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Equations\n",
    "\n",
    "`statsmodels` incorporates `R`-style regression equations by using the `patsy` library behind the scenes. We will talk more about `patsy` soon. The pattern for regression equations is as follows:\n",
    "\n",
    "```\"dependent variable ~ independent variable + another independent variable + any other independent variables\"```\n",
    "\n",
    "The regression equation will be stored in a string (unlike in `R`), and we put our dependent variable (also called the endogenous variable, or outcome of interest) in the leftmost position within the string. We separate the dependent variable from all independent (exogenous or explanatory) variables using the `~` symbol. Then, each independent variable is separated from the others using `+` operators. \n",
    "\n",
    "The reason is is so important that our column names be properly cleaned before implementing regression analysis is that spaces and other problematic formats for column names will cause problems with our regression equations.\n",
    "\n",
    "### Implementing a Model\n",
    "\n",
    "The first model we might try is a simple linear regression. These are the most common regression models, and typically what someone is referring to when they discuss \"running a regression\". The code is wonderfully simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               hhincome   R-squared:                      -0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sat, 22 Nov 2025   Prob (F-statistic):                nan\n",
      "Time:                        14:20:14   Log-Likelihood:            -1.7131e+05\n",
      "No. Observations:               13712   AIC:                         3.426e+05\n",
      "Df Residuals:                   13711   BIC:                         3.426e+05\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
      "year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
      "==============================================================================\n",
      "Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
      "Skew:                           3.151   Prob(JB):                         0.00\n",
      "Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"hhincome ~ year\", data=data).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:               hhincome   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:24:28   Log-Likelihood:            -1.7131e+05\n",
    "    No. Observations:               13712   AIC:                         3.426e+05\n",
    "    Df Residuals:                   13711   BIC:                         3.426e+05\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
    "    year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
    "    ==============================================================================\n",
    "    Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
    "    Skew:                           3.151   Prob(JB):                         0.00\n",
    "    Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "When we run these two lines of code, we are creating, fitting, and reporting on a regression model! It's fast, it's clean, and it's really easy to implement! `sm.ols` is the OLS class of regression models, and takes two required arguments: a regression equation (passed as a string), and a data source (expected to be a `pandas.DataFrame` object). We use the `.fit()` method to complete all of the math that actually solves our regression model. When we call `.summary()` on a fitted regression, we get a printout of the regression summary tables for the model, complete with diagnostic measures, estimates of our beta coefficients, and confidence intervals!\n",
    "\n",
    "If the model is satisfactory, then we are done! (It really is that simple!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to keep iterating on my model, I might want to try regressing year on the logged average household incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sat, 22 Nov 2025   Prob (F-statistic):                nan\n",
      "Time:                        14:21:01   Log-Likelihood:                -17363.\n",
      "No. Observations:               13653   AIC:                         3.473e+04\n",
      "Df Residuals:                   13652   BIC:                         3.474e+04\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
      "year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
      "==============================================================================\n",
      "Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
      "Skew:                          -1.469   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year\", data=data[data['hhincome']>0]).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:31:18   Log-Likelihood:                -17363.\n",
    "    No. Observations:               13653   AIC:                         3.473e+04\n",
    "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
    "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
    "    ==============================================================================\n",
    "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
    "    Skew:                          -1.469   Prob(JB):                         0.00\n",
    "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "As you can see from the code above, everything is the same, except that we were able to transform household income using `np.log` on the go! We don't even need to create a new column! We can just do it inside of our regression model! We also subset our data so that the log operator doesn't break our model by introducing $-\\infty$ as a possible `hhincome` value.\n",
    "\n",
    "In other cases, it might be useful to create state-level fixed effects by including dummy variables for the states in our `statefip` column. Note that this won't work with our current data, since we only have one state in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sat, 22 Nov 2025   Prob (F-statistic):                nan\n",
      "Time:                        14:21:57   Log-Likelihood:                -17363.\n",
      "No. Observations:               13653   AIC:                         3.473e+04\n",
      "Df Residuals:                   13652   BIC:                         3.474e+04\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
      "year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
      "==============================================================================\n",
      "Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
      "Skew:                          -1.469   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data[data['hhincome']>0]).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:32:19   Log-Likelihood:                -17363.\n",
    "    No. Observations:               13653   AIC:                         3.473e+04\n",
    "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
    "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
    "    ==============================================================================\n",
    "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
    "    Skew:                          -1.469   Prob(JB):                         0.00\n",
    "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "The `C()` command indicates that we would like to consider the `statefip` variable as a **C**ategorical variable, not a numeric variable. We can transform ANY column using the categorical operator. It is most useful when a column is text-based, or when a column is numeric but should not be treated as a count, ordinal, or continuous variable. We CAN use it on our dependent variable, but this will (unless our dependent variable was binary text data) break our regression model, which expects only a single dependent variable, rather than an array of dependent variables.\n",
    "\n",
    "Sometimes we want to include transformed variables in our model without creating a new column. The `I()` operator allows us to do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:341: RuntimeWarning: invalid value encountered in dot\n",
      "  beta = np.dot(self.pinv_wexog, self.wendog)\n"
     ]
    }
   ],
   "source": [
    "# Square a variable using the I() function for\n",
    "#   mathematical transformations\n",
    "reg = smf.ols(\"np.log(hhincome) ~ age + I(age**2)\", data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we transform `age` by squaring it (maybe in preparation to create an age-earnings profile?). One line, simple syntax, what could be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:341: RuntimeWarning: invalid value encountered in dot\n",
      "  beta = np.dot(self.pinv_wexog, self.wendog)\n"
     ]
    }
   ],
   "source": [
    "# Combine variables using the I() function for\n",
    "#   mathematical transformations\n",
    "reg = smf.ols(\"np.log(hhincome) ~ I(age-educ-5)\", data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example combines TWO columns to create a new measure (proxying experience by subtracting education from age, and subtracting an additional 5 years). All we have to do is describe the relationship that we want to model as an explanatory variable, and we are off to the races! Most operators are fair game, and we can include an arbitrary number of columns in our measure calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More robust modeling\n",
    "\n",
    "If we want to utilize robust standard errors, we can easily update our regression results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       np.log(hhincome)   R-squared:                         nan\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sat, 22 Nov 2025   Prob (F-statistic):                nan\n",
      "Time:                        14:28:12   Log-Likelihood:                    nan\n",
      "No. Observations:               13702   AIC:                               nan\n",
      "Df Residuals:                   13701   BIC:                               nan\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept        -inf        nan        nan        nan         nan         nan\n",
      "year             -inf        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                     nan\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                  nan\n",
      "Skew:                             nan   Prob(JB):                          nan\n",
      "Kurtosis:                         nan   Cond. No.                     1.66e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.02e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1698: RuntimeWarning: invalid value encountered in subtract\n",
      "  return self.model.wendog - self.model.predict(\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1733: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.sum(weights * (model.endog - mean)**2)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:949: RuntimeWarning: invalid value encountered in subtract\n",
      "  resid = self.endog - np.dot(self.exog, params)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1518: RuntimeWarning: invalid value encountered in subtract\n",
      "  a = op(a[slice1], a[slice2])\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
    "# Use White's (1980) Standard Error\n",
    "reg.get_robustcov_results(cov_type='HC0')\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to cluster our standard errors by state,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1698: RuntimeWarning: invalid value encountered in subtract\n",
      "  return self.model.wendog - self.model.predict(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The weights and list don't have the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m reg \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mols(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.log(hhincome) ~ year + C(statefip)\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdata)\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Use Cluster-robust Standard Errors\u001b[39;00m\n",
      "\u001b[1;32m----> 3\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_robustcov_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatefip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Need to specify groups\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(reg\u001b[38;5;241m.\u001b[39msummary())\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:2601\u001b[0m, in \u001b[0;36mRegressionResults.get_robustcov_results\u001b[1;34m(self, cov_type, use_t, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   2597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adjust_df:\n",
      "\u001b[0;32m   2598\u001b[0m         \u001b[38;5;66;03m# need to find number of groups\u001b[39;00m\n",
      "\u001b[0;32m   2599\u001b[0m         \u001b[38;5;66;03m# duplicate work\u001b[39;00m\n",
      "\u001b[0;32m   2600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_groups \u001b[38;5;241m=\u001b[39m n_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(groups))\n",
      "\u001b[1;32m-> 2601\u001b[0m     res\u001b[38;5;241m.\u001b[39mcov_params_default \u001b[38;5;241m=\u001b[39m \u001b[43msw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_cluster\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_correction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m groups\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;32m   2605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(groups, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\stats\\sandwich_covariance.py:530\u001b[0m, in \u001b[0;36mcov_cluster\u001b[1;34m(results, group, use_correction)\u001b[0m\n",
      "\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    528\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(group)\n",
      "\u001b[1;32m--> 530\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[43mS_crosssection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    532\u001b[0m nobs, k_params \u001b[38;5;241m=\u001b[39m xu\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;32m    533\u001b[0m n_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(clusters) \u001b[38;5;66;03m#replace with stored group attributes if available\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\stats\\sandwich_covariance.py:484\u001b[0m, in \u001b[0;36mS_crosssection\u001b[1;34m(x, group)\u001b[0m\n",
      "\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mS_crosssection\u001b[39m(x, group):\n",
      "\u001b[0;32m    476\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''inner covariance matrix for White on group sums sandwich\u001b[39;00m\n",
      "\u001b[0;32m    477\u001b[0m \n",
      "\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m    I guess for a single categorical group only,\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    482\u001b[0m \n",
      "\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "\u001b[1;32m--> 484\u001b[0m     x_group_sums \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_sums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m#TODO: why transposed\u001b[39;00m\n",
      "\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m S_white_simple(x_group_sums)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\tools\\grouputils.py:107\u001b[0m, in \u001b[0;36mgroup_sums\u001b[1;34m(x, group, use_bincount)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(group) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;32m    103\u001b[0m         group \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mfactorize(group)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n",
      "\u001b[0;32m    106\u001b[0m         [\n",
      "\u001b[1;32m--> 107\u001b[0m             \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    108\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;32m    109\u001b[0m         ]\n",
      "\u001b[0;32m    110\u001b[0m     )\n",
      "\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    112\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(group)\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: The weights and list don't have the same length."
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
    "# Use Cluster-robust Standard Errors\n",
    "reg.get_robustcov_results(cov_type='cluster', groups=data['statefip']) # Need to specify groups\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to stick to just `HC0` and cluster-robust standard errors. Below are some of the [covariance options](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.get_robustcov_results.html) that we have:\n",
    "1) `HC0`: White's (1980) Heteroskedasticity robust standard errors\n",
    "2) `HC1`, `HC2`, `HC3`: MacKinnon and White's (1985) alternative robust standard errors, with `HC3` being designed for improved performance in small samples\n",
    "3) `cluster`: Cluster robust standard errors\n",
    "4) `hac-panel`: Panel robust standard errors\n",
    "\n",
    "We should choose the standard errors that best fit our specific data needs, and it is important to realize that this choice is highly context-dependent. The structure and nature of our data should be carefully considered, as should the specific regression model that we are trying to implement.\n",
    "\n",
    "### Time Series Models\n",
    "\n",
    "Not only can we model linear regression, we also have multiple time series options available. We won't go into much detail, since each of these models deserve to have significant time devoted to them, and we just don't have the time in this class.\n",
    "\n",
    "- [ARIMA](http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html) models\n",
    "- [VAR](http://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.var_model.VAR.html) models\n",
    "- [Exponential Smoothing](https://www.statsmodels.org/stable/tsa.html#exponential-smoothing) models\n",
    "\n",
    "We can run an ARIMA, for example, using code like the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:               hhincome   No. Observations:                13712\n",
      "Model:                 ARIMA(1, 1, 0)   Log Likelihood             -171403.017\n",
      "Date:                Sat, 22 Nov 2025   AIC                         342810.033\n",
      "Time:                        14:32:39   BIC                         342825.085\n",
      "Sample:                             0   HQIC                        342815.049\n",
      "                              - 13712                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ar.L1         -0.1063      0.009    -12.293      0.000      -0.123      -0.089\n",
      "sigma2      4.226e+09   8.28e-13    5.1e+21      0.000    4.23e+09    4.23e+09\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                  19.11   Jarque-Bera (JB):            255880.02\n",
      "Prob(Q):                              0.00   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.31   Skew:                            -0.02\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                        24.16\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "[2] Covariance matrix is singular or near-singular, with condition number    inf. Standard errors may be unstable.\n"
     ]
    }
   ],
   "source": [
    "# This won't work unless we have multiple years of data (which we currently don't)\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "y = data.loc[data['statefip']==31, ['hhincome','year']]\n",
    "y.index=pd.to_datetime(y.year)\n",
    "reg = ARIMA(y['hhincome'], order=(1,1,0)).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Discrete Outcomes\n",
    "\n",
    "If we have a [binary dependent variable](https://www.statsmodels.org/devel/discretemod.html), we are able to use either [Logit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit) or [Probit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Probit.html#statsmodels.discrete.discrete_model.Probit) models to estimate the effect of exogenous variables on our outcome of interest. To fit a Logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.615837\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "myformula=\"married ~ hhincome + C(statefip) + C(year) + educ\"\n",
    "model= sm.Logit.from_formula(myformula, data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Count Data\n",
    "\n",
    "When modeling count data, we have options such as [Poisson](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Poisson.html#statsmodels.discrete.discrete_model.Poisson) and [Negative Binomial](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.NegativeBinomial.html#statsmodels.discrete.discrete_model.NegativeBinomial) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1567: RuntimeWarning: overflow encountered in exp\n",
      "  L = np.exp(np.dot(X,params) + exposure + offset)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1478: RuntimeWarning: overflow encountered in exp\n",
      "  L = np.exp(np.dot(X,params) + offset + exposure)\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/auto-mpg.csv\")\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
    "\n",
    "myformula=\"nchild ~ hhincome + C(statefip) + C(year) + educ + married\"\n",
    "\n",
    "model= sm.Poisson.from_formula(myformula, data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other regression \"flavors\", and the best way to learn about what is available through `statsmodels` is to [read the docs](https://www.statsmodels.org/stable/user-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `patsy` library\n",
    "\n",
    "We have been using regression equations in `statsmodels` a lot without really discussing what is happening behind the scenes. `statsmodels` relies on a library called `patsy` to parse regression equations and prepare our data for regression analysis. While `statsmodels` does a great job of incorporating the `patsy` library for us, this isn't always the case. In fact, it is a really valuable tool in many other contexts (think machine learning or deep learning). \n",
    "\n",
    "\n",
    "### Why use `patsy`?\n",
    "\n",
    "We don't necessarily have to use `patsy`. We could just select our variables manually. Creating a column of ones to serve as our intercept column is trivial (you of course remember that from the linear regression assignment). `patsy` is a tool for creating a standardized pipeline to deal with data that is stored in identical formats, and aids us in creating reusable or replicable code. Patsy allows us to separate our endogenous and exogenous variables AND to\n",
    "\t- \"Dummy out\" categorical variables\n",
    "\t- Easily transform variables (square, or log transforms, etc.)\n",
    "\t- Use identical transformations on future data\n",
    "    \n",
    "Even better, `patsy` is just as easy to use as regression equations. We just need to learn about the function wrappers that are necessary to create our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
    "\n",
    "# To create y AND x matrices\n",
    "y, x = pt.dmatrices(\"hhincome ~ year + educ + married + age\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get started, we need to import `patsy`, and we typically give it the two-letter abbreviation `pt`. Once we have imported our data, we use the `pt.dmatrices` function. This function takes a regression equation (again, as a string), and a data source. The returned value is a **tuple** of `y` and `x`. We can break that tuple into two values by using the `y, x = ...` syntax, so that we have a `y` array and an `x` array.\n",
    "\n",
    "We don't have to create BOTH `y` and `x` data, though! We can use the `pt.dmatrix` function to just create an `x` matrix. Maybe we already have a dependent variable, and want to try out variations on our explanatory variables to see how each performs. In this case, our regression equation should have no column name to the left of the `~` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create ONLY an x matrix\n",
    "x = pt.dmatrix(\"~ year + educ + married + age\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more note is that these regression equations automatically include an intercept term. If you do NOT want an intercept term (some regression models and most machine learning models don't use them), then you can add `-1` as an exogenous variable in your regression equation, in order to indicate that you want to eliminate the column of ones that make up the intercept column in our matrix of exogenous regressors.\n",
    "\n",
    "### Categorical Variables\n",
    "\n",
    "Again, we have the functions described in the regression section above available to us as we transform our data. We can create categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create y AND x matrices\n",
    "eqn = \"hhincome ~ C(year) + educ + married + age\"\n",
    "y, x = pt.dmatrices(eqn, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And (again) we can transform variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# To create y AND x matrices\n",
    "eqn = \"I(np.log(hhincome)) ~ C(year) + educ + married + age + I(age**2)\"\n",
    "y, x = pt.dmatrices(eqn, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use interaction operators. `*` will interact each value of two columns, and also include the original columns in the regression model. `:` will include only the interaction terms, while omitting the original columns. Check out the [explanation of formulas](https://patsy.readthedocs.io/en/latest/formulas.html) for more details.\n",
    "\n",
    "\n",
    "### SUPER IMPORTANT $\\rightarrow$ Same Transformation on New Data!\n",
    "\n",
    "Often, we will want to build a model with observed data that can make predictions about new observations as those observations are recorded. `patsy` provides a simple function to take the structure of one exogenous matrix and generate another identically structured matrix using new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataNew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To create a new x matrix based on our previous version\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m xNew \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mbuild_design_matrices([x\u001b[38;5;241m.\u001b[39mdesign_info], \u001b[43mdataNew\u001b[49m)\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataNew' is not defined"
     ]
    }
   ],
   "source": [
    "# To create a new x matrix based on our previous version\n",
    "xNew = pt.build_design_matrices([x.design_info], dataNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can create a new matrix in the SAME SHAPE as our original `x` matrix by using the `build_design_matrices()` function in `patsy`. \n",
    "\n",
    "We pass a list containing the old design matrix information (because we can actually create many matrices simultaneously), as well as the new data from which to construct our new matrix.\n",
    "\n",
    "Why does recreating our `x` array matter? This process ensures that we always have the same number of categories in our categorical variables. A new, smaller subset of data that is freshly observed may not contain observations of every category, in which case an updated patsy matrix would not contain the correct number of columns! We are able to maintain consistency in our model, making our work replicable. Most importantly, this will streamline the use of `statsmodels` and `sklearn` in the same workflow!\n",
    "\n",
    "Speaking of `sklearn`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`\n",
    "\n",
    "What `statsmodels` does for regression analysis, `sklearn` does for predictive analytics and machine learning. It is a truly fabulous library. `sklearn` is likely the most popular machine learning library, and has a standard API to make using the library VERY simple. Even better, it's documentation is some of the nicest documentation you will find anywhere, and contains incredible detail about how to implement models, as well as lessons about the \"how\" and \"why\" of using each model. You couldn't write a better textbook about machine learning than the documentation for `sklearn`.\n",
    "\n",
    "Below, we will briefly discuss some of the models that are most commonly utilized from `sklearn`. Details will be sparse. We are mostly focused on the code implementation of these models. More detail on how machine learning models work is provided in  Business Forecasting, and is outside the scope of this course.\n",
    "\n",
    "### Decision Tree Classification (and Regression)\n",
    "\n",
    "[Classification](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and [Regression](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) Trees (CARTs) are the standard jumping-off point for exploring machine learning. They are very easy to implement in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 0.9753162225224119\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import patsy as pt\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
    "\n",
    "y, x = pt.dmatrices(\"Occupancy ~ CO2\", data=data)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In-sample accuracy: 0.9753162225224119\n",
    "\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "We also implement [Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html#svm) for both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 0.9184575709198084\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
    "      \"avoid this warning.\", FutureWarning)\n",
    "\n",
    "\n",
    "    In-sample accuracy: 0.9397028122313643\n",
    "\n",
    "\n",
    "Can you see the API pattern yet?\n",
    "\n",
    "### Random Forest Models\n",
    "\n",
    "Again, available in both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) flavors, these models are aggregations of many randomized Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 0.9748250030701215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In-sample accuracy: 0.9748250030701215\n",
    "\n",
    "\n",
    "There MUST be a pattern here...\n",
    "\n",
    "Of course there is! We import our classifier (or regressor), then create an instance of that object. We can name it `clf` or anything else that we prefer. From there, the process is the same:\n",
    "- Use the `.fit()` method, passing in the relevant data for our context\n",
    "- Create predictions using our fitted model with `.predict()` and new exogenous data (or the old data to test in-sample fit)\n",
    "- Measure the performance of our model with `accuracy_score`, or any other metric that can describe performance given a specific use case\n",
    "\n",
    "### More from `sklearn`\n",
    "\n",
    "Many other tools are also available to aid in the data cleaning process through `sklearn`. Some of these are:\n",
    "\n",
    "- [Principal Component Analysis (PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
    "- [Factor Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis)\n",
    "- Many [Cross-Validation Algorithms](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Hyperparameter Tuning](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "   - Finding the correct parameters for a decision tree or random forest, for example\n",
    "- [Model Evaluation Tools](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Plotting decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Using the wage data provided here (https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv), create a linear regression model to explain and/or predict wages. Your data set should be labeled `data` and your fitted model should be stored as `reg`. If you do not name the model correctly, you won't get any points!\n",
    "\n",
    "Please put all your code for this exercise in the cell labeled `#si-linear-regression` file found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               log_wage   R-squared:                       0.553\n",
      "Model:                            OLS   Adj. R-squared:                  0.551\n",
      "Method:                 Least Squares   F-statistic:                     394.4\n",
      "Date:                Sat, 22 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        16:25:37   Log-Likelihood:                -1013.7\n",
      "No. Observations:                4165   AIC:                             2055.\n",
      "Df Residuals:                    4151   BIC:                             2144.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 5.2187      0.063     83.154      0.000       5.096       5.342\n",
      "id                       -0.0002   2.86e-05     -7.255      0.000      -0.000      -0.000\n",
      "year                      0.0906      0.002     37.043      0.000       0.086       0.095\n",
      "years_experience          0.0064      0.000     13.484      0.000       0.006       0.007\n",
      "weeks_worked              0.0047      0.001      4.908      0.000       0.003       0.007\n",
      "occupation_code          -0.1374      0.013    -10.581      0.000      -0.163      -0.112\n",
      "industry_code             0.0541      0.010      5.172      0.000       0.034       0.075\n",
      "south_region             -0.0587      0.011     -5.292      0.000      -0.080      -0.037\n",
      "metropolitan_resident     0.1694      0.011     15.753      0.000       0.148       0.191\n",
      "ms                        0.1063      0.018      5.826      0.000       0.071       0.142\n",
      "female                   -0.3340      0.022    -15.036      0.000      -0.378      -0.290\n",
      "union_member              0.0956      0.011      8.446      0.000       0.073       0.118\n",
      "education                 0.0539      0.002     23.303      0.000       0.049       0.058\n",
      "is_black                 -0.1601      0.020     -8.207      0.000      -0.198      -0.122\n",
      "==============================================================================\n",
      "Omnibus:                      132.753   Durbin-Watson:                   0.597\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              355.446\n",
      "Skew:                          -0.042   Prob(JB):                     6.55e-78\n",
      "Kurtosis:                       4.429   Cond. No.                     4.60e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.6e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#si-linear-regression\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv\")\n",
    "\n",
    "# Model is using all other variables, with log_wage as the dependent.\n",
    "reg = smf.ols(\"log_wage ~ id + year + years_experience + weeks_worked + occupation_code + industry_code + south_region + metropolitan_resident + ms + female + union_member + education + is_black\", data=data).fit()\n",
    "#print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Import the pass/fail data for students in Portugal found here (https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv), and create a logistic regression model \n",
    "using `statsmodels` that can estimate the likelihood of students passing or failing class. The dependent variable is contained in the column called `G3`, which takes the value `1` when the student has a passing final grade, and `0` otherwise.\n",
    "\n",
    "Call your fitted model `reg`, and place all code for this exercise in the cell labeled `#si-logistic-regression` file found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.313777\n",
      "         Iterations: 35\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   No. Observations:                  296\n",
      "Model:                          Logit   Df Residuals:                      263\n",
      "Method:                           MLE   Df Model:                           32\n",
      "Date:                Sun, 23 Nov 2025   Pseudo R-squ.:                  0.4980\n",
      "Time:                        16:13:18   Log-Likelihood:                -92.878\n",
      "converged:                      False   LL-Null:                       -185.01\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.598e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -7.3537   1096.700     -0.007      0.995   -2156.846    2142.139\n",
      "school        -0.2047      0.738     -0.277      0.781      -1.651       1.242\n",
      "sex            0.4474      0.528      0.848      0.397      -0.587       1.482\n",
      "age           -0.6577      0.221     -2.976      0.003      -1.091      -0.225\n",
      "address       -0.2767      0.530     -0.522      0.601      -1.315       0.761\n",
      "famsize       -0.0676      0.456     -0.148      0.882      -0.961       0.826\n",
      "Pstatus       -0.6239      0.680     -0.917      0.359      -1.957       0.709\n",
      "Medu           0.1182      0.292      0.405      0.685      -0.454       0.690\n",
      "Fedu          -0.1619      0.255     -0.634      0.526      -0.662       0.339\n",
      "Mjob          -0.2201      0.187     -1.175      0.240      -0.587       0.147\n",
      "Fjob           0.0371      0.237      0.157      0.875      -0.426       0.501\n",
      "reason         0.1778      0.178      1.000      0.317      -0.171       0.526\n",
      "guardian       0.3291      0.383      0.860      0.390      -0.421       1.079\n",
      "traveltime     0.2020      0.306      0.659      0.510      -0.399       0.803\n",
      "studytime     -0.1881      0.307     -0.613      0.540      -0.790       0.414\n",
      "failures      -0.4727      0.278     -1.703      0.089      -1.017       0.071\n",
      "schoolsup     -0.4990      0.555     -0.899      0.369      -1.587       0.589\n",
      "famsup        -1.0100      0.492     -2.054      0.040      -1.974      -0.046\n",
      "paid           0.7335      0.460      1.596      0.111      -0.167       1.634\n",
      "activities    -0.3238      0.435     -0.745      0.456      -1.176       0.529\n",
      "nursery       -0.4186      0.541     -0.774      0.439      -1.479       0.642\n",
      "higher         1.4184      1.003      1.414      0.157      -0.548       3.385\n",
      "internet      -0.0426      0.534     -0.080      0.936      -1.090       1.004\n",
      "romantic      -0.5544      0.438     -1.266      0.205      -1.412       0.304\n",
      "famrel         0.3450      0.246      1.403      0.161      -0.137       0.827\n",
      "freetime      -0.0287      0.206     -0.139      0.889      -0.432       0.375\n",
      "goout         -0.3706      0.208     -1.785      0.074      -0.777       0.036\n",
      "Dalc          -0.2937      0.273     -1.075      0.283      -0.829       0.242\n",
      "Walc           0.1702      0.240      0.711      0.477      -0.299       0.640\n",
      "health        -0.0632      0.152     -0.416      0.677      -0.361       0.235\n",
      "absences      -0.0435      0.022     -2.013      0.044      -0.086      -0.001\n",
      "G1             3.6036      0.487      7.395      0.000       2.649       4.559\n",
      "G2            18.4843   1096.692      0.017      0.987   -2130.992    2167.960\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandiSnapple\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "#si-logistic-regression\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv\")\n",
    "\n",
    "# Model is using all other variables, with G3 as the dependent.\n",
    "reg = smf.logit(\"G3 ~ school + sex + age + address + famsize + Pstatus + Medu + Fedu + Mjob + Fjob + reason + guardian + traveltime + studytime + failures + schoolsup + famsup + paid + activities + nursery + higher + internet + romantic + famrel + freetime + goout + Dalc + Walc + health + absences + G1 + G2\", data=data).fit()\n",
    "#print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Use the data on NFL franchise values included in the NFL Valuation data source (https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv) file to implement a Random Forest Classifier in sklearn using 100 trees to predict team-years when `Playoffs` takes the value `1` (when a team made the playoffs in that season).\n",
    "\n",
    "- Use Patsy to create `x2` and `y2` matrices\n",
    "- Create the classifier\n",
    "- Fit the classifier, and store the fitted model with the name `playoffForest`\n",
    "\n",
    "Place all code for this exercise in the cell labeled `#si-random-forest` file found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#si-random-forest\n",
    "import patsy as pt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv\")\n",
    "\n",
    "# Create x2 and y2 matrices\n",
    "y2, x2 = pt.dmatrices(\"Playoffs ~ Teamno + Year + Value + ChangeValue + DebtToValue + Revenues + OperatingIncome + Expansion + TVDeal + LaborContract + SuperBowl\", data = data)\n",
    "\n",
    "# Create the classifier (100 trees)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "playoffForest = clf.fit(x2, y2.squeeze())\n",
    "\n",
    "pred = playoffForest.predict(x2)\n",
    "\n",
    "#print(\"In-sample accuracy: {}\".format(accuracy_score(y2.squeeze(), pred)))\n",
    "#print(pred)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
